Supa Eval is a proposed SaaS-first Evaluation Platform designed to address the lack of a unified, standardized way for Agent Builders and LLM teams to evaluate the quality of agents across multiple layers, beyond just the final answer. The platform will enable users to manage datasets, configure and run detailed evaluations across retrieval and generation, visualize metrics through deep-dive and executive dashboards, and implement RLHF (Reinforcement Learning from Human Feedback) for continuous improvement. Key features include Dataset Management, Eval Configuration, Benchmarks, an Evaluation Engine for scalable execution, and an SDK for integration, all aimed at providing quality intelligence and actionable insights to debug and optimize complex agent systems.

Product Requirements Document (PRD)
Product: Universal Agent Evaluation SaaS

1. Overview
   Problem Statement Agent builders and LLM teams lack a unified, standardized way to evaluate quality across datasets, retrieval, generation, and feedback loops. Existing tools are provided only for final answer evaluation, but agents require actionable insights around multiple layers such as Chunking, cracking, retrieval, reranking, generation, context engineering, memory evaluation insights.
   Solution Build a SaaS-first Evaluation Platform that enables teams to:
   Manage and benchmark datasets
   Configure and run evaluations without modifying agent internals
   Visualize metrics and comparisons across the layers
   Easy context feeding, multi-turn, multi- agent support evaluation
   Introduce RLHF loops for continuous improvement
   Integrate via SDKs for advanced workflows
   Target Users
   Agent Builders
   PMs / AI Researchers / Data Scientists
   QA teams

2. Features

I. Data Foundation Layer
(What is evaluated)
2.1 Dataset Management
Description Centralized system to upload, version, tag, and manage evaluation datasets.
Key Capabilities
Dataset types: prompts, conversations, multi-turn tasks
Versioning & immutability
Metadata (domain, difficulty, intent, language)
Train / eval / test splits
Synthetic + human-curated datasets
Non-Goals (v1)
Real-time data labeling UI

2.2 Dataset Marketplace
Description Public and private marketplace for reusable evaluation datasets.
Key Capabilities
Built-in benchmark datasets (general + domain-specific)
Organization-private datasets
Dataset ratings, coverage metadata
Import/export datasets across tenants
Value
Faster onboarding
Shared standards across teams

II. Evaluation Definition Layer
(How evaluation is run)
2.3 Eval Config
Description Declarative configuration defining how an evaluation is executed.
Key Capabilities
Model / agent endpoint config
Prompt templates
Retrieval & tool-call parameters
Sampling strategy
Cost & latency budgets
Versioned configs (Git-like)

2.4 Metrics
Description Standardized and extensible evaluation metrics computed by the platform.
Metric Categories
Retrieval: Precision@K, Recall@K, nDCG
Generation: faithfulness, relevance, hallucination
Task success / pass-fail
Latency & cost metrics
Custom LLM-as-judge metrics
Key Principle ðŸ‘‰ Metrics are computed by the platform, not emitted by customers.

III. Execution & Benchmarking Layer
(How quality is measured)
2.5 Benchmarks
Description Repeatable, comparable evaluation runs against fixed datasets + configs.
Key Capabilities
Named benchmark suites
Baseline vs candidate comparisons
Regression detection
Industry/domain benchmarks
Outputs
Scorecards
Delta reports
Rank ordering of agents/configs

2.6 Evaluation Engine
Description Scalable execution layer that runs evaluations asynchronously.
Key Capabilities
Parallel execution
Partial failure handling
Deterministic re-runs
Secure agent invocation (SaaS-controlled)

IV. Insight & Visualization Layer
(How results are understood)
2.7 Evaluation Dashboards
Description Deep-dive dashboards for analysis and debugging.
Views
Dataset â†’ metric breakdown
Prompt-level failures
Intent vs outcome analysis
Retrieval vs generation attribution
Time-based trends

2.8 Dashboard (Executive / Summary)
Description High-level product and leadership view.
Key Metrics
Overall quality score
Pass rate
Coverage gaps
Regression alerts
Cost vs quality tradeoffs

V. Learning & Optimization Layer
(How systems improve)
2.9 RLHF
Description Feedback loop to improve agents using evaluation outputs.
Key Capabilities
Human or LLM feedback capture
Preference datasets
Reward signal generation
Export to fine-tuning or policy optimization pipelines
Positioning RLHF is downstream of evaluation, not a replacement.

VI. Integration Layer
(How teams adopt)
2.10 SDK
Description Language SDKs enabling programmatic control.
Capabilities
Trigger evaluations
Upload datasets
Fetch metrics
CI/CD integration
Local + hybrid workflows
Supported Languages (v1)
Python
JavaScript / TypeScript

3. Success Metrics
   Time to first evaluation < 4 hours
   Minimal agent code changes required for basic eval
   Benchmark re-run reproducibility > 99%
   Weekly active eval runs per tenant
   Adoption of platform-computed metrics

4. Out of Scope (v1)
   Training foundation models
   Real-time production monitoring/ Online evals
   Prompt IDE / playground

5. Strategic Positioning
   This is not a logging tool. This is not a metrics viewer. This is a quality intelligence platform for agents.
6. User Journey
   User
   â”‚
   â–¼
   Sign Up / Login
   â”‚
   â–¼
   Create Workspace (Org / Project / Environment)
   â”‚
   â–¼
   Choose Evaluation Mode
   â”œâ”€â–º SDK-based Eval
   â””â”€â–º API / Agent Simulator Eval
   â”‚
   â–¼
   Connect Agent
   â”œâ”€â–º Provide Agent Endpoint / Tool APIs
   â”œâ”€â–º Upload Agent Schema / Capabilities
   â””â”€â–º Configure Auth (API Key / OAuth / Token)
   â”‚
   â–¼
   Dataset Setup
   â”œâ”€â–º Upload Custom Prompts
   â”œâ”€â–º Use SupaEval Benchmark Datasets
   â”œâ”€â–º Generate Synthetic Data
   â””â”€â–º Label / Import Ground Truth (Optional)
   â”‚
   â–¼
   Eval Configuration
   â”œâ”€â–º Select Metrics
   â”‚ â€¢ Retrieval (Recall@K, nDCG)
   â”‚ â€¢ Generation (Faithfulness, Hallucination)
   â”‚ â€¢ Tool Use / Agent Success
   â”‚ â€¢ Multi-turn / Long-Horizon
   â”œâ”€â–º Select Judge (LLM / SLM / Hybrid)
   â””â”€â–º Define Pass / Fail Criteria
   â”‚
   â–¼
   Run Evaluation
   â”‚
   â–¼
   Execution Engine
   â”œâ”€â–º Prompt Injection
   â”œâ”€â–º Agent Calls & Tool Traces
   â”œâ”€â–º Context Capture
   â””â”€â–º Output Normalization
   â”‚
   â–¼
   Scoring & Analysis
   â”œâ”€â–º Metric Computation
   â”œâ”€â–º Failure Clustering
   â”œâ”€â–º Root Cause Attribution
   â”‚ â€¢ Retrieval gap
   â”‚ â€¢ Prompt issue
   â”‚ â€¢ Tool failure
   â”‚ â€¢ Hallucination
   â”‚
   â–¼
   Evaluation Dashboard
   â”œâ”€â–º Overall Quality Score
   â”œâ”€â–º Metric Breakdown
   â”œâ”€â–º Prompt-level Drilldown
   â”œâ”€â–º Agent Trace Replay
   â”‚
   â–¼
   Insights & Actions
   â”œâ”€â–º What broke?
   â”œâ”€â–º Why it broke?
   â”œâ”€â–º What to fix first?
   â”‚
   â–¼
   Feedback Loop
   â”œâ”€â–º Export Bad Cases
   â”œâ”€â–º Improve Prompts / Tools / Retrieval
   â”œâ”€â–º RLHF / Fine-tuning (Optional)
   â”‚
   â–¼
   Continuous Evaluation
   â”œâ”€â–º CI/CD Integration
   â”œâ”€â–º Scheduled Runs
   â””â”€â–º Regression Detection
